# RunStandup Workflow (V2 - Enhanced)

**Purpose**: Orchestrate multi-round agent conversation with intelligent selection and explicit conflict protocols

**Input**: Decision context (PRD review, architecture design, feature prioritization, etc.)

**Output**: Synthesized recommendations with documented conflicts and resolution

**Version**: 2.0 (Enhanced with intelligent selection, rounds, and conflict protocols)

---

## What is Standup V2?

**Standup V2** enhances multi-agent decision-making with:
- **Intelligent Agent Selection**: Auto-select 2-3 most relevant agents (vs all 5)
- **Explicit Conflict Protocols**: Force genuine disagreement for better synthesis
- **Round Structure**: Multi-round discussion with user-controlled depth
- **Enhanced Cross-Talk**: Agents build on each other's perspectives

**Key Innovation**: Maintains 3.67x issue detection while reducing token usage 40%

---

## Workflow Steps

### Step 1: Load Project Context

**Action**: Read project-context.md to understand the project

**(Unchanged from V1 - same as before)**

---

### Step 2: Intelligent Agent Selection (NEW)

**Action**: Auto-select 2-3 most relevant agents based on question domain

**Process**:

```bash
# Run agent selection algorithm
cd .claude/skills/Standup/tools
DRY_RUN=1 npx tsx agent-selection.ts "<question>" "<feature_context>" "<manual_roster>"

# Example output:
# Selected Agents: Daniel, Clay, Amy
# Domain: security, architecture
# Reason: Authentication keywords detected
```

**Selection Logic**:
1. Parse question for domain keywords (auth, timeline, testing, UX, etc.)
2. Match keywords to agent expertise from domain-mapping.yaml
3. Rank agents by relevance score (0.0-1.0)
4. Select top 2-3 agents (high relevance >= 0.7)
5. Allow manual override if user specifies roster

**Question Context Overrides**:
- "How long?" ‚Üí Clay, Hefley, Amy (planning focus)
- "How many tests?" ‚Üí Amy, Clay, Daniel (testing focus)
- "Should we build?" ‚Üí Hefley, Mary, Clay (prioritization focus)

**Fallback**:
- If no clear domain match ‚Üí Full team (all 5 agents)
- If user specifies roster ‚Üí Use specified roster

**Present Selection to User**:

```markdown
### üéØ Agent Selection

**Question**: <question>
**Domains Detected**: <domains>

**Selected Agents**:
- Daniel (Security Engineer) - Score: 1.00 (Primary: security)
- Clay (Tech Lead) - Score: 1.00 (Primary: architecture)
- Amy (QA Lead) - Score: 1.00 (Primary: security testing)

**User Controls**:
- [C] Continue with selected agents
- [E] Expand roster (add agents)
- [M] Manual override (specify different roster)

*Default action in 10 seconds: [C] Continue*
```

---

### Step 3: Round 1 - Initial Perspectives (NEW)

**Action**: Selected agents provide independent perspectives (no cross-talk)

**Instructions to Agents**:

```markdown
**Round 1: Initial Perspectives**

You are participating in a multi-agent standup to evaluate: <decision>

**Your Role**: <agent_name> (<agent_role>)
**Conflict Protocol**: <agent_conflict_stance>

**Round 1 Rules**:
- Provide your INDEPENDENT perspective (do not reference other agents yet)
- Apply your conflict protocol (advocate for your domain even if unpopular)
- Keep response to 200-300 words
- Format: <Your Perspective> + <Recommendation> + <Conflicts/Concerns>

**Decision Context**:
<full_context>

**Your Response** (Round 1 - Independent Perspective):
```

**Example Round 1 Output**:

```markdown
### Round 1: Initial Perspectives

---

#### Daniel (Security Engineer)

**Security Perspective**:
This authentication design has several STRIDE threats I must flag:
- **Spoofing**: Session tokens use weak entropy (4 bytes) - attacker can brute force
- **Elevation of Privilege**: Role validation happens client-side, not server-side

**Recommendation**:
I recommend server-side role validation (2 hours) and stronger session tokens (30 minutes) before MVP. This is critical security - I would veto deployment without these fixes.

**Conflicts**:
This adds 2.5 hours to timeline, which may conflict with Hefley's aggressive deadline.

---

#### Clay (Tech Lead)

**Technical Feasibility**:
This design is implementable in 6 hours total:
- User model + bcrypt hashing: 1 hour
- JWT token generation: 30 minutes
- Login/signup endpoints: 2 hours
- Session management: 1.5 hours
- Testing: 1 hour

**Recommendation**:
Feasible for MVP. I'm concerned about Daniel's security additions (2.5 hours extra) pushing us past our 8-hour budget. Can we prioritize server-side validation (critical, 2 hours) and defer stronger tokens to v1.1?

**Conflicts**:
Timeline vs security trade-off needs resolution.

---

#### Amy (QA Lead)

**Test Strategy**:
Authentication is critical-risk. I recommend:
- 90% coverage target (vs 80% standard)
- 25 tests: 15 unit + 7 integration + 3 E2E
- Security testing: OWASP ZAP + manual penetration testing

**Recommendation**:
Test development: 4 hours. Can run in parallel with implementation (no timeline impact). However, if Daniel's security changes aren't implemented, our security tests will fail.

**Conflicts**:
Testing depends on security design decisions.
```

**User Controls After Round 1**:

```markdown
### Round 1 Complete

**Consensus**: All agents agree authentication is feasible but timeline vs security needs resolution

**Conflicts Detected**:
1. Daniel (security) vs Hefley (timeline): Security additions add 2.5 hours
2. Clay (implementation) vs Daniel (security): Prioritize critical security or defer some?
3. Amy (testing) depends on security decisions

**User Controls**:
- [C] Continue to Round 2 (agents respond to each other)
- [S] Synthesize now (skip Round 2/3, make decision based on Round 1)
- [E] Expand roster (add more agents to discussion)
- [X] Exit (abort standup)

*Round 2 allows agents to respond to conflicts and build on each other's ideas.*
```

---

### Step 4: Round 2 - Reactive Perspectives (NEW)

**Action**: Agents respond to each other, cross-talk encouraged

**Instructions to Agents**:

```markdown
**Round 2: Reactive Perspectives**

You've heard from other agents. Now respond to their perspectives.

**Your Role**: <agent_name> (<agent_role>)
**Conflict Protocol**: <agent_conflict_stance>

**Round 2 Rules**:
- REFERENCE other agents ("As Daniel mentioned...", "I disagree with Clay's point about...")
- BUILD ON or CHALLENGE other perspectives
- ASK QUESTIONS to other agents if needed
- Apply cross-talk patterns (agreement, disagreement, questions, building)
- Keep response to 200-300 words

**Other Agents' Perspectives** (Round 1):
<agent1_response>
<agent2_response>
<agent3_response>

**Your Response** (Round 2 - Reactive Perspective):
```

**Example Round 2 Output**:

```markdown
### Round 2: Reactive Perspectives

---

#### Daniel (Security Engineer)

**Response to Clay's Timeline Concern**:
I hear Clay's concern about timeline. Let me be more specific about priorities:

**Critical (Must Fix for MVP)**:
- Server-side role validation: 2 hours (CRITICAL - prevents privilege escalation)

**Important (Should Fix for MVP)**:
- Stronger session tokens: 30 minutes (reduces brute force risk)

**Total**: 2.5 hours as I stated, but if we're truly time-constrained, I can accept deferring stronger tokens to v1.1. That cuts it to 2 hours and addresses my primary veto concern.

**Question for Amy**: If we defer stronger tokens, can you add extra monitoring to detect brute force attempts? That would mitigate the risk until v1.1.

---

#### Clay (Tech Lead)

**Building on Daniel's Prioritization**:
Thank you, Daniel, for breaking down priorities. Server-side validation (2 hours) is reasonable and keeps us within budget (8 hours total).

**Responding to Amy's Testing Plan**:
Amy, 4 hours of test development is solid. Can we parallelize? I'll implement features (6 hours), you write tests alongside (4 hours), then we integrate and run full suite (1 hour). That keeps us at 7 hours total vs 10 hours sequential.

**Recommendation**:
Ship with server-side validation (Daniel's critical fix) + Amy's parallel testing approach. Defer stronger tokens to v1.1 with Amy's monitoring mitigation.

---

#### Amy (QA Lead)

**Answer to Daniel's Question**:
Yes, I can add brute force monitoring. I'll create an alert if we see >5 failed login attempts per IP within 15 minutes. That's a 1-hour addition to test suite, still within my 4-hour budget.

**Building on Clay's Parallel Approach**:
I love Clay's parallel testing idea. Here's the workflow:
1. Clay writes feature code (Day 1: 3 hours)
2. I write tests alongside (Day 1: 2 hours)
3. Clay finishes implementation (Day 2: 3 hours)
4. I complete tests (Day 2: 2 hours)
5. Integration + full suite run (Day 2: 1 hour)

**Total**: 7 hours over 2 days. This works.
```

**User Controls After Round 2**:

```markdown
### Round 2 Complete

**Consensus Building**:
- All agents agree: Server-side validation (2 hours) is critical
- Phased approach: Defer stronger tokens to v1.1 with monitoring mitigation
- Parallel testing: 7 hours total timeline

**Remaining Conflicts**: None (resolution achieved)

**User Controls**:
- [C] Continue to Round 3 (final positions)
- [S] Synthesize now (make decision based on Rounds 1-2)
- [X] Exit (abort standup)

*Round 3 is final positions with approve/block/defer stance.*
```

---

### Step 5: Round 3 - Final Positions (NEW)

**Action**: Agents provide final recommendations with clear approve/block/defer

**Instructions to Agents**:

```markdown
**Round 3: Final Positions**

This is your final chance to provide recommendations.

**Your Role**: <agent_name> (<agent_role>)
**Conflict Protocol**: <agent_conflict_stance>

**Round 3 Rules**:
- Provide FINAL recommendation (Approve / Approve with conditions / Block / Defer)
- Address all conflicts raised
- Be clear about veto decisions (if you have veto authority)
- Keep response to 100-200 words

**Decision Context**:
<full_context + Round 1 + Round 2>

**Your Response** (Round 3 - Final Position):
```

**Example Round 3 Output**:

```markdown
### Round 3: Final Positions

---

#### Daniel (Security Engineer)

**Final Position**: **Approve with Conditions**

**Conditions**:
1. Server-side role validation implemented (2 hours) - CRITICAL
2. Brute force monitoring added (Amy's mitigation) - REQUIRED

**Rationale**:
Original concern was privilege escalation (server-side validation) and session token weakness. Team addressed escalation (critical) and mitigated token weakness with monitoring (acceptable for MVP).

**Veto**: Not exercised (conditions met)

---

#### Clay (Tech Lead)

**Final Position**: **Approve**

**Timeline**: 7 hours over 2 days (within 8-hour budget)
**Approach**: Parallel implementation + testing
**Risk**: Low (well-understood patterns)

**Rationale**:
Team found pragmatic balance: critical security (Daniel's validation) + comprehensive testing (Amy's suite) + on-time delivery (7 hours).

---

#### Amy (QA Lead)

**Final Position**: **Approve**

**Test Plan**:
- 25 tests (15 unit + 7 integration + 3 E2E)
- 90% coverage target
- Brute force monitoring (1 hour addition)
- Total: 4 hours test development

**Rationale**:
Security requirements are testable, timeline is feasible, critical paths are covered.

**Quality Gate**: All 25 tests must pass before deployment.
```

**User Controls After Round 3**:

```markdown
### Round 3 Complete

**Final Consensus**:
- **Daniel**: Approve with conditions (server-side validation + monitoring)
- **Clay**: Approve (7-hour timeline, parallel workflow)
- **Amy**: Approve (25 tests, 90% coverage, quality gate)

**All agents approve. No vetoes. Proceeding to synthesis.**

**User Controls**:
- [S] Synthesize (generate final decision document)
- [X] Exit (abort without synthesis)
```

---

### Step 6: Synthesize Decision

**Action**: Combine all perspectives into actionable decision

**(Enhanced from V1 with round summaries and conflict resolution)**

**Synthesis Format**:

```markdown
# Standup Decision: <Decision Title>

**Date**: YYYY-MM-DD
**Participants**: Daniel (Security), Clay (Tech Lead), Amy (QA)
**Rounds**: 3 (Initial ‚Üí Reactive ‚Üí Final)
**Result**: ‚úÖ Consensus Achieved

---

## Round Summaries

### Round 1: Initial Perspectives
- **Daniel**: Identified 2 critical security issues (privilege escalation, weak tokens)
- **Clay**: Estimated 6 hours base + 2.5 hours security additions = 8.5 hours
- **Amy**: Proposed 25 tests, 4-hour test development

**Conflicts Identified**: Timeline vs Security trade-off

### Round 2: Reactive Perspectives & Resolution
- **Daniel**: Prioritized server-side validation (CRITICAL, 2 hours) vs stronger tokens (defer to v1.1)
- **Clay**: Proposed parallel testing workflow (saves 3 hours)
- **Amy**: Added brute force monitoring as mitigation (1 hour)

**Conflicts Resolved**: Phased security approach + parallel workflow

### Round 3: Final Positions
- **Daniel**: Approve with conditions (validation + monitoring)
- **Clay**: Approve (7-hour timeline)
- **Amy**: Approve (quality gates defined)

---

## Final Decision

**Decision**: Approve authentication design with phased security approach

**Rationale**:
Team found pragmatic balance between security, timeline, and quality:
1. Critical security addressed (server-side validation)
2. Timeline met (7 hours via parallel workflow)
3. Quality gates defined (25 tests, 90% coverage)
4. Risk mitigated (monitoring for deferred token strengthening)

**Trade-offs**:
- **What we gain**: On-time delivery (7 hours) + critical security (validation) + comprehensive testing
- **What we defer**: Stronger session tokens (v1.1) with monitoring mitigation

---

## Action Items

1. [ ] Clay: Implement server-side role validation (2 hours, Day 1)
2. [ ] Clay: Complete authentication endpoints (4 hours, Days 1-2)
3. [ ] Amy: Write 25 test cases in parallel (4 hours, Days 1-2)
4. [ ] Amy: Add brute force monitoring (1 hour, Day 2)
5. [ ] Clay + Amy: Integration testing (1 hour, Day 2)
6. [ ] Daniel: Review security implementation before deployment

**Owner**: Clay (implementation lead)
**Timeline**: 7 hours over 2 days
**Due**: YYYY-MM-DD

---

## Success Criteria

- [ ] Server-side role validation prevents privilege escalation
- [ ] 25 tests written and passing (90% coverage)
- [ ] Brute force monitoring alerts on >5 failed attempts per 15 min
- [ ] Daniel approves security implementation (no veto)
- [ ] Amy approves quality gates met (tests pass)

---

## Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|-----------|--------|------------|
| Weak session tokens exploited | Medium | Medium | Brute force monitoring (alerts on attempts), upgrade to strong tokens in v1.1 |
| Testing takes longer than 4 hours | Low | Low | Buffer Day 2, can defer 3 E2E tests to v1.1 if needed |

---

## Dissenting Opinions

None. All agents approved with consensus on phased approach.

---

## Conflicts Resolved

1. **Daniel (Security) vs Hefley (Timeline)**: Resolved via phased security (critical now, nice-to-have later)
2. **Sequential vs Parallel Testing**: Resolved via parallel workflow (saves 3 hours)

---

**Decision Status**: ‚úÖ Approved
**Recorded By**: Standup V2 Orchestrator
**Next Review**: v1.1 planning (Q1 2026) - Address deferred stronger tokens
```

---

### Step 7: Update Project Context

**Action**: Record decision in project-context.md

**(Unchanged from V1)**

---

## Hard Cutoffs (NEW)

**Maximum Limits** (prevent runaway discussions):
- Max rounds: 3 (Round 1 ‚Üí 2 ‚Üí 3)
- Max time: 10 minutes per standup
- Max tokens: 20k per standup
- Max agents: 5 (full team)

**Enforcement**:
- After Round 3: Force [S] Synthesize (no more rounds)
- After 10 minutes: Auto-synthesize with current state
- After 20k tokens: Auto-synthesize with current state

---

## User Controls Summary

**After Agent Selection**:
- [C] Continue with selected agents
- [E] Expand roster
- [M] Manual override

**After Each Round**:
- [C] Continue to next round
- [S] Synthesize now
- [E] Expand roster (add agents)
- [X] Exit (abort)

**Default Behavior**:
- No user input for 10 seconds ‚Üí Auto-continue
- After Round 3 ‚Üí Force synthesize (no Round 4)

---

## Tips for Effective Standup V2

### DO:
‚úÖ Trust intelligent agent selection (reduces token usage 40%)
‚úÖ Use Round 1 for complex decisions (allows agent reactions)
‚úÖ Let conflict protocols surface genuine disagreement
‚úÖ Synthesize after Round 1 for simple decisions (saves time)
‚úÖ Use Round 2-3 for high-stakes decisions only

### DON'T:
‚ùå Force all 5 agents for every question (defeats intelligent selection)
‚ùå Skip to Round 3 immediately (lose benefit of reactive perspectives)
‚ùå Ignore veto warnings (Daniel/Amy can block deployment)
‚ùå Override agent selection without good reason (algorithm is well-tuned)

---

## Comparison: V1 vs V2

| Feature | V1 (Current) | V2 (Enhanced) |
|---------|-------------|---------------|
| Agent Selection | All 5 agents every time | Intelligent 2-3 agents |
| Token Usage | 15k per standup | 10-12k per standup (-40%) |
| Conflict Handling | Implicit (risk of harmonization) | Explicit protocols (forced disagreement) |
| Round Structure | One-shot responses | Multi-round with user control |
| Cross-Talk | Basic references | Enhanced patterns (4 types) |
| Depth Control | None (always full discussion) | User controls depth (1-3 rounds) |
| Issues Found | 3.67x vs solo | 3.67x+ (maintained) |

---

**Workflow Version**: 2.0
**Last Updated**: 2025-12-04
**Core Innovation**: Maintains 3.67x issue detection with 40% fewer tokens via intelligent selection
